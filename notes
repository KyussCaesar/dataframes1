what would be neat:

    - familiar feeling to working with dataframes/tibbles in R.
    - but make use of compiler type-checking where possible.

important invariants

    - all columns are the same length
    - all rows are the same length

R functions I care about

    - select
    - mutate
    - gather
    - spread
    - group_by
    - summarise
    - filter
    - rbind
    - cbind

# `select`: Returns a new dataframe with only the columns specified.

Accepts a dataframe with some columns, returns a dataframe with a subset of those
columns.

## Invariants:

Input and output have same number of rows.
Columns in output are subset of columns in input.


# `mutate`: Add or modify columns in dataframe.

## Invariants:

Input and output have same number of rows.
Columns in output are superset of the columns in the input.


# `gather`: Collect multiple columns into key-value pairs.

In other words, convert from "wide" format into "long" format.

## Invariants:

If an operation collapses C columns of a dataframe that has N rows, the output
will have NC rows.


# `spread`: Opposite of gather.

Arguments are "id", "key" and "value".
Result creates a new column for each level in "key", for each "id".

## Invariants:


# `group_by`: Group by levels of a factor.

## Invariants:

Grouping by a factor with M levels results in M groups.


# `summarise`: Reduce over groups.

## Invariants:

For a dataframe that was grouped by a factor with M levels, the dataframe resulting
from this operation will have M rows.


# `filter`: Return rows which satisfy a predicate.

## Invariants:

The rows in the result are a subset of the rows in the input.
The columns in the output are the same as those in the input.


# `rbind`: Concatenate rows of two dataframes.

## Invariants:

The columns in both arguments must be the same.
For inputs with M and N rows, the output will have M+N rows.


# `cbind`: Concatenate columns of two dataframes.

## Invariants:

The number of rows in both arguments must be the same.
The number of rows in the output is the same the number of rows in the input.
For inputs with M and N columns, the output will have M+N columns.


# Dynamic vs Static

Would be nice to catch as many things at compile-time as is possible.
Imagine if you had this code running in a container somewhere, you just throw data
at it and expect it not to fail.

Will probably have to trade-off somewhere.
For example; can define custom types, which means accessing a non-existent column
is a compile-error, but then you cannot add columns without defining a new type for
it.

Possibly, you could use macros to make this a bit less tedious.
For example, what about a macro which wraps a struct definition, and results
in defining a dataframe type?

On the other hand, you could implement DataFrame like a hashmap between column
names (String) and a Vec<T>. This allows dynamically adding and removing columns, 
as well as renaming them, but at the expense of loosing compile-time checking for
things like non-existent columns.

I had imagined that the thing I evenually design would be aimed at people who
already know the computation they want to do, so the columns required would
essentially be static.

Something that would be annoying would be generating an intermediate type for
every intermediate value.
Possible intermediate is to make columns Option<Column>, and leave room for
future expansion.
For example, if you add some columns, make the dataframe type

    struct DataFrame
    {
        a: Column<A>,
        b: Option<Column<B>>,
    }

Accessing a column which does not exist becomes compile-time error, but accessing
a column before it is initialised would still be a runtime-error.
Maybe this isn't really much better :\

---

one, general-purpose `pipe` function, and macros which populate it with the correct
arguments.

---

would be nice to find some way of automatically generating the intermediate types.
macros would be able to do this.

could perhaps return struct of closures.... no that won't work, still need to
know the type of the value coming in.

would be cool if rust allowed type inference on struct definition for stuff like
this.

what would be perfect would be tuples with named fields.

what about struct around a tuple, and a mapping between 'static str <=> number in tuple

might be able to use enums to provide semi-dynamic typing

build an enum that knows about a bunch of types, and a generic constructor which
is specialised for each type, returning a different variant depending on the type?

inside macro expansion, create new type:

struct NewRecord
{
    $($item: TypeWrapper),*
}

where

enum TypeWrapper
{
    Type(val)....
}

impl Into<TypeWrapper> for EACH_T
{
    fn into(self)
    {
        TypeWrapper::EACH_T_VARIANT(self)
    }
}

select!(a, b, c) ->
|r|
{
    struct NewRecord
    {
        a: TypeWrapper,
        b: TypeWrapper,
        c: TypeWrapper,
    }

    NewRecord { a: r.a.into(), b: r.b.into(), c: r.c.into() }
}

add `DataFrame::pipe` as a generic method which accepts some kind of weird thing
and a specifier for the op; use macros to fill arguments

...almost, but now everything after this point is dynamically typed

could enforce the definition of a type alias for each field in the record,
then you can talk about the type of field `foo` by ::foo

could use a proc macro to do the above, replace

    struct Record
    {
        foo: f32,
    }

with

    type foo = f32;
    struct Record
    {
        foo: ::foo;
    }

don't necessarily need to replace type definition

with this, macro can expand to

|r|
{
    struct NewRecord
    {
        $($id : ::$id),*
    }

    NewRecord { $($id),* }
}

e.g select!(a, b, c)

    struct NewRecord
    {
        a: ::a,
        b: ::b,
        c: ::c,
    }

    NewRecord { a, b, c }

Might not work because of macro hygeine, instead, define and impl a custom
trait which has an associated type?

struct Record
{
    a: f32,
    b: usize,
}

macro would define trait

pub trait __Record_Trait
{
    type a_t;
    type b_t;
}

impl __Record_Trait for Record
{
    type a_t = f32;
    type b_t = usize;
}

in other macros, can refer to this type as

<Record as __Record_Trait>::a_t

for a, and likewise for b.

less flexible, try aliases first; this method doesn't work after one `select` because
you have no way to talk about the type, and therefore to retrieve the type of
it's fields.

check if this works

fn makes<a_T, b_T, c_T, d_T, T>(a: A, b: B, c: C, d: D) -> (Fn(A, B, C, D) -> T)
{
    struct NewType
    {
        a: A,
        b: B,
        c: C,
        d: D,
    }

    return |aa, bb, cc, dd| NewType { a: aa, b: bb, c: cc, d: dd }
}

if so, then make a macro which expands to this function, and then you can use
it to create value of your new type

even simpler would be

macro_rules! makes
{
    ( $($item:ident),* ) =>
    {
        fn makes_new<F>() -> F
        {
            // define new type
            struct NewType
            {
                $($item : $item_T),*
            }

            // define function which stamps out values of that new type and return it
            return |$($item),*| NewType { $($item),* }
        }

        // create function which creates values of the new type and yield as
        // result of macro invocation
        makes_new()
    }
}

doesn't work; you can't leak type from the function

---

what about something similar to the iterator adapters?
